{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfe4f7f-6557-419f-afe8-6e13a1a68eb5",
   "metadata": {},
   "source": [
    "## LiteLLM API workflow\n",
    "\n",
    "Examples and use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131f45f7-59a2-4b3c-b8ec-d33688cf4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# You need a file config.py with a variable called api_key containing your key\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f7e353-3807-4011-9c7d-0dfb3e6a5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232299ba",
   "metadata": {},
   "source": [
    "Which models are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eeaa6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai-research-proxy.azurewebsites.net/models\"\n",
    "r = requests.get(url=url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf6422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n",
      "text-embedding-ada-002\n",
      "gpt-4.1\n",
      "Llama-3.3-70B-Instruct\n",
      "Llama-3.2-90B-Vision-Instruct\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for m in r.json().get(\"data\")[:5]:\n",
    "    print(m.get(\"id\"))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9acd40",
   "metadata": {},
   "source": [
    "Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b91a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373bff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompts = [\n",
    "    \"Write a haiku about recursion in programming.\",\n",
    "    \"Write a haiku about functional programming.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d210ba9-36e7-4877-80d0-ab31cb085d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai-research-proxy.azurewebsites.net/chat/completions\"\n",
    "\n",
    "responses = []\n",
    "for prompt in sample_prompts:\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": persona},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    responses.append(requests.post(url=url, data=json.dumps(data), headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8669c5a-cbe3-415d-aafd-71d2d041ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "In code’s endless flow,  \n",
      "Functions call their own whispers,  \n",
      "Infinity loops.\n"
     ]
    }
   ],
   "source": [
    "print(responses[0].status_code)\n",
    "print(responses[0].json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f8715-ba7d-49ee-8448-b4f80bb2afdf",
   "metadata": {},
   "source": [
    "### Other client libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66763254",
   "metadata": {},
   "source": [
    "LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm  # you need to pip install this external library first\n",
    "# https://docs.litellm.ai/docs/providers/litellm_proxy#send-all-sdk-requests-to-litellm-proxy\n",
    "\n",
    "litellm.api_base = \"https://ai-research-proxy.azurewebsites.net/\"\n",
    "litellm.api_key = api_key\n",
    "litellm.use_litellm_proxy = True\n",
    "\n",
    "# Non-streaming response\n",
    "response = litellm.responses(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Tell me a three sentence bedtime story about a unicorn.\",\n",
    "    max_output_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GenericResponseOutputItem(type='message', id='chatcmpl-CG4dpdMuWbOdYI3IsZ3LLCvdvHR9S', status='stop', role='assistant', content=[OutputText(type='output_text', text=\"Once upon a time in a shimmering forest, a gentle unicorn named Luna discovered a hidden glade filled with sparkling flowers that glowed softly under the moonlight. She made a wish for all the lost creatures of the night to find solace and warmth, and as she did, the flowers sprang to life, illuminating the path for all to follow. From that night on, the glade became a magical sanctuary where all beings felt safe and loved, thanks to Luna's kindness.\", annotations=[])])]\n"
     ]
    }
   ],
   "source": [
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0c634",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e66856-0da7-45ab-bf63-3af557b382fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # you need to pip install this external library first\n",
    "# https://docs.litellm.ai/docs/proxy/user_keys#chatcompletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f345e2-351b-4843-81fa-47ec87972705",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://ai-research-proxy.azurewebsites.net/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7229b9c7-8b58-4c85-8c83-a6ce7b008925",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for prompt in sample_prompts:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": persona},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edea262d-f5cf-471b-83f2-98fe1f63dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Code calls itself back,  \\nLayers of thought intertwine—  \\nEndless loops of dreams.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0].choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d64bc9-1924-40bb-ba20-c50fc1d898c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
